{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this notebook is to perform training for few steps and inference given a pretrained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from hydra import compose, initialize\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from hydra.utils import instantiate\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.img import unnormalize\n",
    "from utils.training import get_batch, get_dataloaders\n",
    "from utils.visualization import plot_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = str(Path().absolute())\n",
    "\n",
    "# Initialize Hydra manually for Jupyter Notebook\n",
    "if not GlobalHydra.instance().is_initialized():\n",
    "    initialize(config_path=\"config\", version_base=None)\n",
    "\n",
    "# Load configuration and overrides elements\n",
    "overrides = [\"val_dataloader.batch_size=1\", f\"project_root={project_root}\"]\n",
    "cfg = compose(config_name=\"base\", overrides=overrides)\n",
    "\n",
    "# Set seed\n",
    "seed = 0\n",
    "print(f\"Seed: {seed}\")\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Backbones\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "backbone = instantiate(cfg.backbone)\n",
    "backbone.to(device)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, val_dataloader = get_dataloaders(\n",
    "    cfg, backbone, is_evaluation=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = instantiate(cfg.model)\n",
    "model.cuda()\n",
    "\n",
    "optimizer = instantiate(cfg.optimizer, params=list(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "model.train()\n",
    "criterion = instantiate(cfg.loss, dim=backbone.embed_dim)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Initialize progress bar with additional formatting\n",
    "    pbar = tqdm(train_dataloader, desc=f\"Epoch {epoch}\", leave=True, postfix={\"loss\": \"...\"})\n",
    "\n",
    "    for step, batch in enumerate(pbar):\n",
    "        batch = get_batch(batch, device)\n",
    "        image_batch = batch[\"image\"]\n",
    "        hr_feats, _ = backbone(image_batch)\n",
    "\n",
    "        low_res_batch = F.interpolate(image_batch, scale_factor=0.5, mode=\"area\")\n",
    "        lr_feats, _ = backbone(low_res_batch)\n",
    "\n",
    "        lr_img_batch = F.interpolate(image_batch, hr_feats.shape[-2:])\n",
    "        pred = model(lr_img_batch, lr_feats, (hr_feats.shape[2], hr_feats.shape[3]))\n",
    "\n",
    "        loss_hr = criterion(hr_feats, pred)[\"total\"]\n",
    "        loss = loss_hr\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update progress bar with current loss\n",
    "        pbar.set_postfix({\"loss_hr\": f\"{loss_hr.item():.4f}\"})\n",
    "\n",
    "        if step % 500 == 0:\n",
    "            unorm_img_batch = unnormalize(image_batch, backbone.config[\"mean\"], backbone.config[\"std\"])\n",
    "            plot_feats(\n",
    "                unorm_img_batch[0].to(torch.float32),\n",
    "                hr_feats[0].to(torch.float32),\n",
    "                pred[0].to(torch.float32),\n",
    "            )\n",
    "        if step == 2000:\n",
    "            break\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        \"./output/jafar/vit_small_patch14_dinov2.lvd142m/model.pth\"\n",
    "    )[\"jafar\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "SIZE = 448\n",
    "with torch.no_grad():\n",
    "    for step, batch in enumerate(\n",
    "        tqdm(val_dataloader)\n",
    "    ):\n",
    "        batch = get_batch(batch, device)\n",
    "        image_batch = batch[\"image\"].cuda()\n",
    "        bs = image_batch.shape[0]\n",
    "\n",
    "        hr_feats, _ = backbone(image_batch)\n",
    "        pred = model(image_batch, hr_feats, (SIZE,SIZE))\n",
    "\n",
    "        unorm_img_batch = unnormalize(\n",
    "            image_batch, backbone.config[\"mean\"], backbone.config[\"std\"]\n",
    "        )\n",
    "        plot_feats(\n",
    "            unorm_img_batch[0].to(torch.float32),\n",
    "            torch.nn.functional.interpolate(hr_feats, SIZE, mode=\"bilinear\")[0].to(torch.float32),\n",
    "            pred[0].to(torch.float32),\n",
    "        )\n",
    "        break\n",
    "    \n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
